Take 2 on the Indexer.

The overall idea is:

1. Take a subset of data, split it up into words, build an in-memory
description.

2. Merge that subset with the disk index. While merging check some of
the elements encountered for presense in the actuald data set. Remember
those checked and those that need to be deleted.

3. Mark the subset as processed. Get next subset if required and
continue at 1.

==========

While doing (2) word by word we check word count and mark the word as
soft- or hard- ignorable. Hard-ignorable are not even stored into the
index, just marked in the list of ignored words. Soft ignorable.. well,
probably there is nothing about them at indexing stage -- we only
care when we get to the search stage and ignore them unless they are
specified in quotes.

The resulting sets are stored in a couple of slots, with a set maximum
number of entries per slot.

HavingZZ


